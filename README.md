# GANs for Image Generation

## Objective

To train a Generative Adversarial Network (GAN) capable of generating new, realistic images that resemble images from the CIFAR10 dataset.

## Dataset Used:

**CIFAR-10:** A dataset of color images representing 10 categories of objects.

  * **Source:** Canadian Institute for Advanced Research (CIFAR); commonly available through deep learning libraries like torchvision, TensorFlow Datasets, etc.

  * **Description:** Contains 60,000 32x32 color images in 10 classes, with 6,000 images per class.

  * **Training Set:** 50,000 images.

  * **Test Set:** 10,000 images.

  * **Features:** 32x32 pixels, with 3 color channels (Red, Green, Blue) representing pixel intensities.

  * **Classes:** 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).

## Theory:

Generative Adversarial Networks (GANs) employ a game-theoretic approach to generative modeling. They consist of two core neural networks trained simultaneously in competition:

1.  **The Generator (G):** This network's goal is to create synthetic data (in this case, images) that is indistinguishable from real data. It typically takes a random noise vector (latent variable) as input and transforms it through several layers (often using upsampling or transposed convolutions) to produce an image of the desired dimensions. The generator learns by receiving feedback from the discriminator. Its objective is to produce outputs that the discriminator incorrectly classifies as 'real'.
2.  **The Discriminator (D):** This network acts as a binary classifier. Its goal is to distinguish between real images (coming from the actual dataset) and fake images (produced by the generator). It takes an image as input and outputs a probability indicating whether the image is real (probability close to 1) or fake (probability close to 0). It is trained on both real examples (labeled as 'real') and fake examples generated by G (labeled as 'fake').

### Training Dynamics:

The training process involves an adversarial game:

  * The discriminator is trained to improve its ability to correctly classify real and fake images. Its loss function penalizes misclassifications.
  * The generator is trained to fool the discriminator. Its loss function penalizes the discriminator correctly identifying its output as fake. Effectively, the generator aims to maximize the discriminator's classification error on fake images.

This competition drives both networks to improve. As the discriminator gets better at spotting fakes, the generator must produce increasingly realistic images to fool it. The ideal outcome is reaching a Nash equilibrium where the generator produces perfect replicas of the data distribution, and the discriminator can only guess randomly (outputting 0.5 probability).

### Key Monitoring Aspects:

  * **Loss Functions:** Monitoring the loss of both the generator and discriminator is crucial. Typically, Binary Cross-Entropy loss is used. Ideally, the losses should stabilize, indicating a balance in training. If one loss drops to zero while the other explodes, it often signifies a problem like the generator overpowering the discriminator or vice-versa (or mode collapse, where the generator only produces a limited variety of outputs).
  * **Generated Images:** Periodically visualizing the images produced by the generator from fixed noise vectors provides a qualitative assessment of learning progress. Early in training, images will look like noise, but over time, they should gain structure and realism resembling the training dataset.

## Code:

```python
import torch
import torchvision
from torch import nn
from torch import optim
from torchvision.datasets import CIFAR10 # Changed from FashionMNIST
from torchvision import transforms
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# Set random seed for reproducibility
torch.manual_seed(42)

# Define hyperparameters and variables
LEARNING_RATE = 0.0005
BATCH_SIZE = 1024
IMAGE_SIZE = 64 # Kept 64x64 for consistency with network architecture
EPOCHS = 50
image_channels = 3 # Changed from 1 to 3 for RGB
noise_channels = 256
gen_features = 64
disc_features = 64

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the transform
# Normalize for 3 channels
data_transforms = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

# Load the dataset - Changed to CIFAR10
dataset = CIFAR10(root="dataset/", train=True, transform=data_transforms, download=True)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

# Generator
class Generator(nn.Module):
    def __init__(self, noise_channels, image_channels, features):
        super(Generator, self).__init__()
        # The generator model uses 4 ConvTranspose blocks. Each block contains a ConvTranspose2d, BatchNorm2d, and ReLU activation.
        self.model = nn.Sequential(
            # Transpose block 1: Input: noise_channels x 1 x 1 -> Output: (features*16) x 4 x 4
            nn.ConvTranspose2d(noise_channels, features * 16, kernel_size=4, stride=1, padding=0),
            nn.ReLU(),
            # Transpose block 2: Input: (features*16) x 4 x 4 -> Output: (features*8) x 8 x 8
            nn.ConvTranspose2d(features * 16, features * 8, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(features * 8),
            # Transpose block 3: Input: (features*8) x 8 x 8 -> Output: (features*4) x 16 x 16
            nn.ConvTranspose2d(features * 8, features * 4, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(features * 4),
            # Transpose block 4: Input: (features*4) x 16 x 16 -> Output: (features*2) x 32 x 32
            nn.ConvTranspose2d(features * 4, features * 2, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(features * 2),
            # Last transpose block: Input: (features*2) x 32 x 32 -> Output: image_channels x 64 x 64
            nn.ConvTranspose2d(features * 2, image_channels, kernel_size=4, stride=2, padding=1),
            nn.Tanh(),
        )

    def forward(self, x):
        return self.model(x)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self, image_channels, features):
        super(Discriminator, self).__init__()
        # The discriminator model has 5 Conv blocks with Conv2d, BatchNorm, and LeakyReLU activation.
        self.model = nn.Sequential(
            # Conv block 1: Input: image_channels x 64 x 64 -> Output: features x 32 x 32
            nn.Conv2d(image_channels, features, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            # Conv block 2: Input: features x 32 x 32 -> Output: (features*2) x 16 x 16
            nn.Conv2d(features, features * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 2),
            nn.LeakyReLU(0.2),
            # Conv block 3: Input: (features*2) x 16 x 16 -> Output: (features*4) x 8 x 8
            nn.Conv2d(features * 2, features * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 4),
            nn.LeakyReLU(0.2),
            # Conv block 4: Input: (features*4) x 8 x 8 -> Output: (features*8) x 4 x 4
            nn.Conv2d(features * 4, features * 8, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(features * 8),
            nn.LeakyReLU(0.2),
            # Conv block 5: Input: (features*8) x 4 x 4 -> Output: 1 x 1 x 1
            nn.Conv2d(features * 8, 1, kernel_size=4, stride=1, padding=0),
            nn.Sigmoid(),
        )

    def forward(self, x):
        output = self.model(x)
        return output.view(-1) # Flatten to [batch_size]

# Load models
gen_model = Generator(noise_channels, image_channels, gen_features).to(device)
disc_model = Discriminator(image_channels, disc_features).to(device)

# Setup optimizers
gen_optimizer = optim.Adam(gen_model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))
disc_optimizer = optim.Adam(disc_model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))

# Define loss function
criterion = nn.BCELoss()

# Set models to training mode
gen_model.train()
disc_model.train()

# Define labels with smoothing
fake_label = 0.1
real_label = 0.9

# Define fixed noise for consistent image generation
fixed_noise = torch.randn(64, noise_channels, 1, 1).to(device)

# TensorBoard writers - Changed paths
writer_real = SummaryWriter(f"runs/cifar10/test_real")
writer_fake = SummaryWriter(f"runs/cifar10/test_fake")

# Loss tracking
gen_losses = []
disc_losses = []

# Training loop
print("Start training...")
step = 0

for epoch in range(EPOCHS):
    for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=f"Epoch {epoch}")):
        data = data.to(device)
        batch_size = data.shape[0]

        # Train discriminator on real data
        disc_model.zero_grad()
        label = (torch.ones(batch_size) * real_label).to(device)
        output = disc_model(data).reshape(-1)
        real_disc_loss = criterion(output, label)
        d_x = output.mean().item()

        # Train discriminator on fake data
        noise = torch.randn(batch_size, noise_channels, 1, 1, device=device)
        fake = gen_model(noise)
        label = (torch.ones(batch_size) * fake_label).to(device)
        output = disc_model(fake.detach()).reshape(-1)
        fake_disc_loss = criterion(output, label)

        # Calculate discriminator loss
        disc_loss = real_disc_loss + fake_disc_loss
        disc_loss.backward()
        disc_optimizer.step()

        # Train generator
        gen_model.zero_grad()
        label = torch.ones(batch_size).to(device)
        output = disc_model(fake).reshape(-1)
        gen_loss = criterion(output, label)
        gen_loss.backward()
        gen_optimizer.step()

        # Store losses
        gen_losses.append(gen_loss.item())
        disc_losses.append(disc_loss.item())

        # Log losses and images
        if batch_idx % 50 == 0:
            step += 1
            print(
                f"Epoch: {epoch} ===== Batch: {batch_idx}/{len(dataloader)} "
                f"Disc loss: {disc_loss:.4f} ===== Gen loss: {gen_loss:.4f}"
            )
            with torch.no_grad():
                fake_images = gen_model(fixed_noise)
                # Use normalize=True for displaying in TensorBoard
                img_grid_real = torchvision.utils.make_grid(data[:40], normalize=True)
                img_grid_fake = torchvision.utils.make_grid(fake_images[:40], normalize=True)
                writer_real.add_image("Real images", img_grid_real, global_step=step)
                writer_fake.add_image("Generated images", img_grid_fake, global_step=step)

    # Save generated images every 10 epochs or at epoch 1
    if (epoch + 1) % 10 == 0 or epoch == 0:
        with torch.no_grad():
            fake_images = gen_model(fixed_noise).detach().cpu()
            plt.figure(figsize=(10, 10))
            for j in range(64):
                plt.subplot(8, 8, j + 1)
                # For CIFAR10 (RGB), no cmap='gray'
                img = fake_images[j].permute(1, 2, 0).numpy() # Change from (C, H, W) to (H, W, C) for matplotlib
                img = (img * 0.5) + 0.5 # Denormalize from [-1, 1] to [0, 1]
                img = np.clip(img, 0, 1) # Clip values to be within [0, 1]
                plt.imshow(img)
                plt.axis('off')
            plt.savefig(f'generated_images_epoch_{epoch + 1}.png')
            plt.close()

# Plot losses
plt.figure(figsize=(10, 5))
plt.plot(gen_losses, label='Generator Loss')
plt.plot(disc_losses, label='Discriminator Loss')
plt.title('Generator and Discriminator Losses During Training')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.savefig('loss_plot.png')
plt.close()

```

## Output:

```
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to dataset/cifar-10-python.tar.gz
100%
170498071/170498071 [00:14<00:00, 11.7MB/s]
Extracting dataset/cifar-10-python.tar.gz to dataset/
Start training...
Epoch 0:   0%|          | 0/49 [00:00<?, ?it/s]
Epoch: 0 ===== Batch: 0/49 ===== Disc loss: 1.3885 ===== Gen loss: 7.0089
Epoch 0: 100%|██████████| 49/49 [01:32<00:00,  1.89s/it]
Epoch 1:   0%|          | 0/49 [00:00<?, ?it/s]
Epoch: 1 ===== Batch: 0/49 ===== Disc loss: 1.4055 ===== Gen loss: 2.1357
Epoch 1: 100%|██████████| 49/49 [01:32<00:00,  1.89s/it]
Epoch 2:   0%|          | 0/49 [00:00<?, ?it/s]
Epoch: 2 ===== Batch: 0/49 ===== Disc loss: 1.3393 ===== Gen loss: 1.8326
... (output continues for subsequent epochs) ...
Epoch 47:  0%|          | 0/49 [00:00<?, ?it/s]
Epoch: 47 ===== Batch: 0/49 ===== Disc loss: 1.1802 ===== Gen loss: 1.2381
Epoch 47: 100%|██████████| 49/49 [01:32<00:00,  1.89s/it]
Epoch 48:  0%|          | 0/49 [00:00<?, ?it/s]
Epoch: 48 ===== Batch: 0/49 ===== Disc loss: 1.1799 ===== Gen loss: 1.1772
Epoch 48: 100%|██████████| 49/49 [01:32<00:00,  1.89s/it]
Epoch 49:  0%|          | 0/49 [00:00<?, ?it/s]
Epoch: 49 ===== Batch: 0/49 ===== Disc loss: 1.1794 ===== Gen loss: 1.1814
Epoch 49: 100%|██████████| 49/49 [01:32<00:00,  1.89s/it]

```

## Generator and Discriminator Losses During Training
![loss_plot.png](assets/loss_plot.png?raw=true "loss_plot")
![generated_images_epoch_1.png](assets/generated_images_epoch_1.png?raw=true "generated_images_epoch_1.png")
![generated_images_epoch_10.png](assets/generated_images_epoch_10.png?raw=true "generated_images_epoch_10.png")
![generated_images_epoch_20.png](assets/generated_images_epoch_20.png?raw=true "generated_images_epoch_20.png")
![generated_images_epoch_30.png](assets/generated_images_epoch_30.png?raw=true "generated_images_epoch_30.png")
![generated_images_epoch_40.png](assets/generated_images_epoch_40.png?raw=true "generated_images_epoch_40.png")
![generated_images_epoch_50.png](assets/generated_images_epoch_50.png?raw=true "generated_images_epoch_50.png")

### Conclusion:

A successful experiment will yield a generator network that has learned the underlying patterns and distribution of the CIFAR10 dataset. This demonstrates the GAN's ability to perform generative modelling, creating new, plausible images that share the characteristics of the real data, even though they are not exact copies of any training examples. The quality of the generated images and the stability of the loss curves indicate the success level.